{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Dataset Tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'hello',\n",
       " 1: 'world',\n",
       " 2: 'how',\n",
       " 3: 'are',\n",
       " 4: 'you',\n",
       " 5: 'good',\n",
       " 6: 'morning',\n",
       " 7: 'night',\n",
       " 8: 'bye'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_tokenized = {\n",
    "    \"hello\": 0, \"world\": 1, \"how\": 2, \"are\": 3, \"you\": 4,\n",
    "    \"good\": 5, \"morning\": 6, \"night\": 7, \"bye\": 8\n",
    "}\n",
    "\n",
    "reverse_vocab_tokenized = {idx: word for word, idx in vocab_tokenized.items()}\n",
    "\n",
    "reverse_vocab_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class TextGeneratorModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size=32, h_size=128, max_seq_length=10):\n",
    "        super(TextGeneratorModel, self).__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=emb_size)\n",
    "        self.lstm1 = nn.LSTM(input_size=emb_size, hidden_size=h_size, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=h_size, hidden_size=h_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(h_size, 64)\n",
    "        self.fc2 = nn.Linear(64, vocab_size)\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "vocab_size = len(vocab_tokenized)\n",
    "model = TextGeneratorModel(vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "adam_optim = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0],\n",
       "         [2, 3],\n",
       "         [5, 0],\n",
       "         [5, 0],\n",
       "         [0, 0]]),\n",
       " tensor([1, 4, 6, 7, 8]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# tokenize training data\n",
    "training_sentences = [\n",
    "    [\"hello\", \"world\"],\n",
    "    [\"how\", \"are\", \"you\"],\n",
    "    [\"good\", \"morning\"],\n",
    "    [\"good\", \"night\"],\n",
    "    [\"bye\"]\n",
    "]\n",
    "\n",
    "tokenized_train_sentences = [[vocab_tokenized[word] for word in seq] for seq in training_sentences]\n",
    "\n",
    "# retrieve X_train and y_train\n",
    "X_train = []\n",
    "y_train = []\n",
    "for seq in tokenized_train_sentences:\n",
    "    X_train.append(seq[:-1])\n",
    "    y_train.append(seq[-1])\n",
    "\n",
    "# uniforme the sequences dimension\n",
    "max_seq_length = max(len(seq) for seq in X_train)\n",
    "for seq in X_train:\n",
    "    while len(seq) < max_seq_length:\n",
    "        seq.append(0)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, loss: 0.5551\n",
      "Epoch 99, loss: 0.5547\n",
      "Epoch 149, loss: 0.5545\n",
      "Epoch 199, loss: 0.5545\n",
      "Epoch 249, loss: 0.5549\n",
      "Epoch 299, loss: 0.5545\n",
      "Epoch 349, loss: 0.5547\n",
      "Epoch 399, loss: 0.5545\n",
      "Epoch 449, loss: 0.5545\n",
      "Epoch 499, loss: 0.5595\n"
     ]
    }
   ],
   "source": [
    "# epoch training loop\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    adam_optim.zero_grad()\n",
    "    predictions = model(X_train)\n",
    "    loss = criterion(predictions, y_train)\n",
    "    loss.backward()\n",
    "    adam_optim.step()\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch}, loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_word(text):\n",
    "    sentence = [vocab_tokenized[word] for word in text.split() if word in vocab_tokenized]\n",
    "    while len(sentence) < max_seq_length:\n",
    "        sentence.append(0)\n",
    "    input = torch.tensor([sentence], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        output = model(input)\n",
    "        word_index = torch.argmax(output, dim=1).item()\n",
    "    word = reverse_vocab_tokenized[word_index]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello ->  bye\n",
      "good -> morning\n",
      "how are -> you\n"
     ]
    }
   ],
   "source": [
    "print(\"hello -> \", generate_next_word(\"hello\"))\n",
    "print(\"good ->\", generate_next_word(\"good\"))\n",
    "print(\"how are ->\", generate_next_word(\"how are\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
